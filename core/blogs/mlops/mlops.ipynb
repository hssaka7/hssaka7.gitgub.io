{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"ML OPS Ideas\"\n",
    "\n",
    "description: \"ML ops idea and principles\"\n",
    "author: \"Aakash Basnet\"\n",
    "date: \"2024/03/04\"\n",
    "page-layout: full\n",
    "categories:\n",
    "  - keras\n",
    "  - teserflow\n",
    "  - AI\n",
    "  - python\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering Pipelines / Data pipelines\n",
    "The machine learning model is as good as the data. Hence, it is important to set up the data piplelines for data extraction, exploration, validation, wrangling/cleaning and splitting the raw data for making it machine learning ready. The objective of data pipelines are to perform operation to create training and testing datasets.\n",
    "\n",
    "## Data Extraction\n",
    "Extract data from datbases, API, message queuing systems, web or network locations.\n",
    "## Data Validation\n",
    "Validate data range, data schemas\n",
    "## Data Preprocessing\n",
    "Clean, transform data,\n",
    "## Feature Engineering\n",
    "Extract the feature from the datasets.\n",
    "\n",
    "It is best practice to version conteo the the outupt of this pipeline for reproducability and governence(auditing).To version control Features, feature Registery is needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Registry:\n",
    "Features registry/store are backend where features are stored along with their timestap tracking every entries creation and uptades. \n",
    "\n",
    "## Level 1:\n",
    "A simple feature registry could be maintained using a relational database design by adding the feature to track the time.\n",
    "\n",
    "## Level 2:\n",
    "As the data moves from stuctured to unstructured, it is good idea to save the data as objecs. Save the snapshot of feautures in backend(AWS S3, shared network locations)\n",
    "\n",
    "## Level 3: \n",
    "Use data lake data versioning tool like Data Version Control(DVC). Is there existing DVC used in the datalake?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning (ML) pipelines\n",
    "\n",
    "## Feature Extraction\n",
    "## Feature Preprocess \n",
    "feature transformation, splitting data to training, testing and validation sets.\n",
    "## Model Training\n",
    "## Model Testing\n",
    "## Model Evaluation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Registry\n",
    "We can searilize the model files and use the same concept as feature registry to store and version control the model images and artifacts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Serving\n",
    "Model serving could be the multiple combination based on the nature of model serving. \n",
    "\n",
    "## Level1: Batch processig jobs\n",
    "Batch processing python scripts can be scheduled to checkout the model from model registry and make predictions by feeding the data to the models. For, Tabaleu reporting , the python scripts can be scheduled to connect to  Tabaleu server and write predictions. The structure of the batch scripts looks like below: \n",
    "- Extract the data to make predictions\n",
    "- Transform the data using feature engineering pipeline script\n",
    "- Checkout the serving model from model registery\n",
    "- Make predictions\n",
    "- Transform the prediction to make it ready for datbase.\n",
    "- Update the database with pediction value\n",
    "- Provide the feedback and store the new features to feature registry\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Level2: API \n",
    "The model can be packaged and served as API(Model as a Service). User can call API for and post their data for prediction. The API end point will trigger the pipeline make predictions. The batch processing job could still call the API to make predictions. API could be integrated to databases, API, microservices, tabaleu, excel. APIs can be served as server side templating(JinJa, HTML, markdowns) or whole fornt end(React, Anguler, Vue)\n",
    "\n",
    "\n",
    "\n",
    "### Options:\n",
    "- FAST API: Rapid Development, Asyncronys design, auto documentation and data validation through pydantic data models, server side templating compatible.\n",
    "\n",
    "- FLASK: Has more support community and is in market for longer time, server side templating compatible.\n",
    "\n",
    "\n",
    "## Level 3: Adding message queueing system to the API backed.\n",
    "\n",
    "### Options:\n",
    "- Kakfa: \n",
    "\n",
    "- Amazon SQS:\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model monitoring "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
