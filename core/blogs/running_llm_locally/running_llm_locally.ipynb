{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Running LLM locally\"\n",
    "\n",
    "description: \"Setting up open soure llm locally and builing simple application on top of it\"\n",
    "author: \"Aakash Basnet\"\n",
    "date: \"2025/01/12\"\n",
    "page-layout: full\n",
    "categories:\n",
    "  - llm\n",
    "  - gemini\n",
    "  - AI\n",
    "  - python\n",
    "format:\n",
    "  html:\n",
    "    code-fold: false\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![LLM and AI (Generated by Imagen3)](llm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "On your terminal run the following command to install ollama module.\n",
    "```\n",
    "pip install ollama\n",
    "\n",
    "```\n",
    "Pull and run the model of you choice. For this tutorial we are running Gemma2 2B model, since it is small and powerful\n",
    "```\n",
    "ollama pull gemma2:2b\n",
    "```\n",
    "\n",
    "Install Langchain community module\n",
    "```\n",
    "pip install langchain-ollama\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Model\n",
    "For this experiment we will loading gemma2:2b using ollama and langchain model.\n",
    "The temperature is set to 0. Temperature near to 0 makes llm output more deterministic. For application executing certain task the temperature should be set near to 0. But for task like generating poem or movie scricpt the temperature should be set near to 1. The temperature near to 1 makes llm more creative and add ramdomness element to the process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.llms import OllamaLLM\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "llm = OllamaLLM(model=\"gemma2:2b\",\n",
    "                temperature = 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets build LLM agent to translate one language to another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('```json\\n'\n",
      " '{\\n'\n",
      " '  \"English\": \"I love grilled chicken\",\\n'\n",
      " '  \"Spanish\": \"¡Me encanta el pollo a la parrilla!\",\\n'\n",
      " '  \"German\": \"Ich liebe gegrillte Hähnchen!\",\\n'\n",
      " '  \"Korean\": \"닭갈비를 좋아해요!\" \\n'\n",
      " '}\\n'\n",
      " '```')\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"\n",
    "            You are a helpful assistant that translates {input_language} to {output_language}. \n",
    "            Provides output in json format as language as key and outpus as value\n",
    "            \"\"\",\n",
    "        ),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "ai_message = chain.invoke(\n",
    "    {\n",
    "        \"input_language\": \"English\",\n",
    "        \"output_language\": \"Spanish, German, Korean\",\n",
    "        \"input\": \"I love grilled chicken\",\n",
    "    }\n",
    ")\n",
    "\n",
    "pprint(ai_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The llm model response is in string format. Now, lets load the string response as json object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'English': 'I love grilled chicken',\n",
      " 'German': 'Ich liebe gegrillte Hähnchen!',\n",
      " 'Korean': '닭갈비를 좋아해요!',\n",
      " 'Spanish': '¡Me encanta el pollo a la parrilla!'}\n"
     ]
    }
   ],
   "source": [
    "# Replace and assign back to original content\n",
    "ai_message = ai_message.replace(\"```json\", \"\")\n",
    "ai_message = ai_message.replace(\"```\", \"\")\n",
    "\n",
    "# Don't forget to convert to JSON as it is a string right now:\n",
    "json_result = json.loads(ai_message)\n",
    "pprint(json_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
